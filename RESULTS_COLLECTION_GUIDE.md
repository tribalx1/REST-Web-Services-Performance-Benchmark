# Guide de Collecte des R√©sultats - Benchmark REST

## üìã Ordre d'Ex√©cution

### √âtape 1: Tests JMeter (T2, T4, T5)

Pour chaque variante (A, C, D), ex√©cutez les 4 sc√©narios:

```powershell
# D√©marrer une variante
.\RUN_VARIANT.ps1 C  # Changez pour A ou D

# Dans un autre terminal, lancez les tests JMeter
cd jmeter

# Sc√©nario 1: READ-heavy
jmeter -n -t scenario-1-read-heavy.jmx `
  -JHOST=localhost -JPORT=8082 `
  -l results/C-read-heavy.jtl `
  -e -o results/C-read-heavy-report

# Sc√©nario 2: JOIN-filter
jmeter -n -t scenario-2-join-filter.jmx `
  -JHOST=localhost -JPORT=8082 `
  -l results/C-join-filter.jtl `
  -e -o results/C-join-filter-report

# Sc√©nario 3: MIXED
jmeter -n -t scenario-3-mixed.jmx `
  -JHOST=localhost -JPORT=8082 `
  -l results/C-mixed.jtl `
  -e -o results/C-mixed-report

# Sc√©nario 4: HEAVY-body
jmeter -n -t scenario-4-heavy-body.jmx `
  -JHOST=localhost -JPORT=8082 `
  -l results/C-heavy-body.jtl `
  -e -o results/C-heavy-body-report
```

### √âtape 2: Collecter M√©triques JVM via Prometheus (T3)

Pendant que les tests tournent, ouvrez Grafana ou Prometheus:

**Grafana**: http://localhost:3000 (admin/admin)

**Requ√™tes Prometheus** √† ex√©cuter sur http://localhost:9090 :

```promql
# CPU Process Usage (%)
rate(process_cpu_usage[5m]) * 100

# Heap Memory (Mo)
jvm_memory_used_bytes{area="heap"} / 1024 / 1024

# GC Time (ms/s)
rate(jvm_gc_pause_seconds_sum[1m]) * 1000

# Threads actifs
jvm_threads_live

# HikariCP connections actives
hikaricp_connections_active

# HikariCP connections max
hikaricp_connections_max
```

---

## üìä T2 - Extraction depuis JMeter Report

Ouvrez le rapport HTML: `jmeter/results/C-read-heavy-report/index.html`

### Dans "Statistics" tab:

| M√©trique JMeter | Colonne Tableau T2 |
|-----------------|-------------------|
| Throughput (req/sec) | RPS |
| 50th pct | p50 (ms) |
| 95th pct | p95 (ms) |
| 99th pct | p99 (ms) |
| Error % | Err % |

### Template T2:

```
READ-heavy | Variant C
- RPS: _____ req/s
- p50: _____ ms
- p95: _____ ms
- p99: _____ ms
- Err%: _____%

JOIN-filter | Variant C
- RPS: _____ req/s
- p50: _____ ms
- p95: _____ ms
- p99: _____ ms
- Err%: _____%

MIXED | Variant C
- RPS: _____ req/s
- p50: _____ ms
- p95: _____ ms
- p99: _____ ms
- Err%: _____%

HEAVY-body | Variant C
- RPS: _____ req/s
- p50: _____ ms
- p95: _____ ms
- p99: _____ ms
- Err%: _____%
```

R√©p√©tez pour Variants A et D.

---

## üîç T3 - M√©triques JVM (Prometheus/Grafana)

### M√©thode 1: Via Grafana Dashboard

1. Cr√©ez un dashboard avec ces panels
2. S√©lectionnez la p√©riode du test
3. Notez moyenne (avg) et pic (max)

### M√©thode 2: Via Prometheus Query

Pendant le test, ex√©cutez sur http://localhost:9090/graph :

```promql
# CPU moyen sur 10 min
avg_over_time(process_cpu_usage{application="variant-c-springmvc"}[10m]) * 100

# CPU pic
max_over_time(process_cpu_usage{application="variant-c-springmvc"}[10m]) * 100

# Heap moyen (Mo)
avg_over_time(jvm_memory_used_bytes{area="heap",application="variant-c-springmvc"}[10m]) / 1024 / 1024

# Heap pic (Mo)
max_over_time(jvm_memory_used_bytes{area="heap",application="variant-c-springmvc"}[10m]) / 1024 / 1024
```

### Template T3:

```
Variant A (JAX-RS):
- CPU: avg ___%, pic ___% 
- Heap: avg ___ Mo, pic ___ Mo
- GC time: avg ___ ms/s, pic ___ ms/s
- Threads: avg ___, pic ___
- Hikari: actifs ___, max 20

Variant C (Spring MVC):
- CPU: avg ___%, pic ___% 
- Heap: avg ___ Mo, pic ___ Mo
- GC time: avg ___ ms/s, pic ___ ms/s
- Threads: avg ___, pic ___
- Hikari: actifs ___, max 20

Variant D (Spring Data REST):
- CPU: avg ___%, pic ___% 
- Heap: avg ___ Mo, pic ___ Mo
- GC time: avg ___ ms/s, pic ___ ms/s
- Threads: avg ___, pic ___
- Hikari: actifs ___, max 20
```

---

## üìç T4 - D√©tails par Endpoint (JOIN-filter)

Dans le rapport JMeter `C-join-filter-report/index.html`:

Allez dans l'onglet **"Statistics"** et filtrez par Transaction Name:

### Endpoints √† chercher:

1. `GET /items?categoryId=X` ou `GET /items/by-category/X`
2. `GET /categories/{id}/items`

Pour chaque endpoint, notez:
- Throughput ‚Üí RPS
- 95th pct ‚Üí p95
- Error % ‚Üí Err %

### Template T4:

```
Sc√©nario JOIN-filter

GET /items?categoryId=
- Variant A: RPS ___, p95 ___ ms, Err ___%, Notes: ___
- Variant C: RPS ___, p95 ___ ms, Err ___%, Notes: ___
- Variant D: RPS ___, p95 ___ ms, Err ___%, Notes: ___

GET /categories/{id}/items:
- Variant A: RPS ___, p95 ___ ms, Err ___%, Notes: ___
- Variant C: RPS ___, p95 ___ ms, Err ___%, Notes: ___
- Variant D: RPS ___, p95 ___ ms, Err ___%, Notes: ___
```

**Observations √† noter**:
- Utilisation de JOIN FETCH? (Regardez les logs Hibernate si `show_sql=true`)
- Probl√®me N+1? (Nombre de requ√™tes SQL pour 1 requ√™te HTTP)
- Format r√©ponse? (JSON simple vs HAL/HATEOAS)

---

## üìç T5 - D√©tails par Endpoint (MIXED)

Dans le rapport JMeter `C-mixed-report/index.html`:

### Endpoints MIXED:

1. GET /items
2. POST /items
3. PUT /items/{id}
4. DELETE /items/{id}
5. GET /categories
6. POST /categories

Pour chaque endpoint √ó variante:
- Throughput ‚Üí RPS
- 95th pct ‚Üí p95
- Error % ‚Üí Err %

### Template T5:

```
Sc√©nario MIXED

GET /items:
- A: RPS ___, p95 ___ ms, Err ___%
- C: RPS ___, p95 ___ ms, Err ___%
- D: RPS ___, p95 ___ ms, Err ___%

POST /items:
- A: RPS ___, p95 ___ ms, Err ___%
- C: RPS ___, p95 ___ ms, Err ___%
- D: RPS ___, p95 ___ ms, Err ___%

PUT /items/{id}:
- A: RPS ___, p95 ___ ms, Err ___%
- C: RPS ___, p95 ___ ms, Err ___%
- D: RPS ___, p95 ___ ms, Err ___%

DELETE /items/{id}:
- A: RPS ___, p95 ___ ms, Err ___%
- C: RPS ___, p95 ___ ms, Err ___%
- D: RPS ___, p95 ___ ms, Err ___%

GET /categories:
- A: RPS ___, p95 ___ ms, Err ___%
- C: RPS ___, p95 ___ ms, Err ___%
- D: RPS ___, p95 ___ ms, Err ___%

POST /categories:
- A: RPS ___, p95 ___ ms, Err ___%
- C: RPS ___, p95 ___ ms, Err ___%
- D: RPS ___, p95 ___ ms, Err ___%
```

---

## ‚ö†Ô∏è T6 - Incidents / Erreurs

### O√π chercher les erreurs:

1. **Rapports JMeter**: Section "Errors" dans les rapports HTML
2. **Logs applications**: 
   ```powershell
   # Dans le terminal o√π tourne la variante
   # Chercher les exceptions, stack traces
   ```
3. **Logs PostgreSQL**:
   ```powershell
   docker logs benchmark-postgres | findstr ERROR
   ```

### Template T6:

```
Run 1 | Variant C | Timeout HTTP | 2.3% | Connection pool satur√© | Augmenter HikariCP max
Run 2 | Variant D | 500 Internal | 0.8% | NullPointerException | Fix code ligne X
```

---

## üìä T7 - Synth√®se

### Calcul des √©carts:

```
√âcart % = ((Meilleur - Moins bon) / Moins bon) * 100
```

**Exemple**:
- Variant C: 850 RPS
- Variant D: 650 RPS
- √âcart: ((850-650)/650)*100 = 30.8% plus rapide

### Template T7:

```
D√âBIT GLOBAL (RPS):
- Meilleure variante: ___
- √âcart: ___% plus rapide que ___
- Commentaires: ___

LATENCE P95:
- Meilleure variante: ___
- √âcart: ___ ms de diff√©rence vs ___
- Commentaires: ___

STABILIT√â (erreurs):
- Meilleure variante: ___
- √âcart: ___% vs ___% d'erreurs
- Commentaires: ___

EMPREINTE CPU/RAM:
- Meilleure variante: ___
- √âcart: ___% moins de CPU, ___ Mo moins de RAM
- Commentaires: ___

FACILIT√â D'APPROCHE RELATIONNELLE:
- Meilleure variante: ___
- Observations: JOIN FETCH natif? N+1 √©vit√©? HAL overhead?
- Commentaires: ___
```

---

## üéØ Checklist Compl√®te

- [ ] T0: Configuration document√©e ‚úÖ (d√©j√† fait)
- [ ] T1: Sc√©narios d√©finis ‚úÖ (d√©j√† d√©finis)
- [ ] T2: Tests JMeter ex√©cut√©s pour les 3 variantes √ó 4 sc√©narios
- [ ] T3: M√©triques JVM collect√©es via Prometheus
- [ ] T4: D√©tails JOIN-filter extraits
- [ ] T5: D√©tails MIXED extraits
- [ ] T6: Incidents document√©s
- [ ] T7: Synth√®se comparative r√©dig√©e

---

## üöÄ Commandes Rapides

```powershell
# D√©marrer les services
cd monitoring
docker-compose up -d

# Variante A
.\RUN_VARIANT.ps1 A
# Test sur http://localhost:8081

# Variante C
.\RUN_VARIANT.ps1 C
# Test sur http://localhost:8082

# Variante D
.\RUN_VARIANT.ps1 D
# Test sur http://localhost:8083

# Arr√™ter proprement
# Ctrl+C dans le terminal de la variante

# V√©rifier les services
docker ps
curl http://localhost:8082/actuator/health
```
