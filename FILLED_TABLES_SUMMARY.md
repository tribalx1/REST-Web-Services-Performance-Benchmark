# ğŸ“Š Benchmark Tables - Summary of Filled Data

**Date**: November 8, 2025  
**Tests Completed**: Variant A & C (READ-heavy scenario)  
**Status**: Partial - Variant D pending

---

## âœ… What's Been Filled

### T0 â€” Configuration (100% Complete)
- âœ… All system information filled
- CPU, RAM, OS, Java, Docker versions documented

### T2 â€” Results by Scenario (20% Complete)
**Filled for READ-heavy scenario:**
- âœ… Variant A (Jersey): 122.25 RPS, p50=140ms, p95=251ms, p99=331ms, 0% errors
- âœ… Variant C (Spring MVC): 90.74 RPS, p50=142ms, p95=431ms, p99=814ms, 0% errors
- âŒ Variant D: Not tested yet
- âŒ Other scenarios (JOIN-filter, MIXED, HEAVY-body): Not tested yet

### T5 â€” Endpoint Details (30% Complete)
**Filled for GET requests:**
- âœ… GET /items - A: 61.14 RPS, p95=305ms | C: 45.39 RPS, p95=564ms
- âœ… GET /categories - A: 61.47 RPS, p95=210ms | C: 45.50 RPS, p95=230ms
- âŒ POST, PUT, DELETE operations: Not tested yet
- âŒ Variant D: Not tested yet

### T7 â€” Synthesis (50% Complete)
**Filled with A vs C comparison:**
- âœ… Throughput: A wins (+34.7%)
- âœ… Latency p95: A wins (-41.8%)
- âœ… Latency p99: A wins (-59.3%)
- âœ… Stability: Both equal (0% errors)
- âŒ CPU/RAM metrics: Need Grafana data
- âŒ Complete analysis: Need Variant D

---

## ğŸ”´ What's Still Missing

### T1 â€” Scenarios Definition
- âš ï¸ Need to verify actual test configuration matches planned scenarios
- Current test used: 20 threads, 30s ramp-up, 5 min duration
- Original plan: 50â†’100â†’200 threads, 60s ramp-up, 10 min duration

### T2 â€” Complete Scenario Results
**Need to run these tests:**
- âŒ JOIN-filter scenario (items?categoryId queries)
- âŒ MIXED scenario (GET/POST/PUT/DELETE mix)
- âŒ HEAVY-body scenario (5KB payloads)
- âŒ All scenarios for Variant D

### T3 â€” JVM Resources (0% Complete)
**Need from Grafana/Prometheus:**
- âŒ CPU usage (avg/peak) for each variant
- âŒ Heap memory (avg/peak) for each variant
- âŒ GC time (avg/peak) for each variant
- âŒ Active threads (avg/peak) for each variant
- âŒ HikariCP connections (active/max) for each variant

**How to get this data:**
1. Open Grafana: http://localhost:3000 (admin/admin)
2. While test is running, note the metrics from dashboards
3. Or query Prometheus: http://localhost:9090
   - `process_cpu_usage`
   - `jvm_memory_used_bytes{area="heap"}`
   - `jvm_gc_pause_seconds_sum`
   - `jvm_threads_live`
   - `hikaricp_connections_active`

### T4 â€” JOIN-filter Details (0% Complete)
**Need to test:**
- âŒ GET /items?categoryId= endpoint performance
- âŒ GET /categories/{id}/items endpoint performance
- âŒ All three variants
- âŒ Observations on JOIN strategy, N+1 queries, projections

### T5 â€” MIXED Scenario Details (30% Complete)
**Still need:**
- âŒ POST /items performance
- âŒ PUT /items/{id} performance
- âŒ DELETE /items/{id} performance
- âŒ POST /categories performance
- âŒ Variant D for all operations

### T6 â€” Incidents/Errors (0% Complete)
**Good news:** No errors in current tests (0%)
- But should document any issues encountered during testing
- Example: Variant C had latency spikes (max 10.4s)

---

## ğŸ¯ Next Steps to Complete Tables

### Immediate Priority (High Value):

1. **Get Grafana Metrics for A & C** (15 minutes)
   - Open Grafana while variants are running
   - Screenshot or note down the values
   - Fill T3 table

2. **Test Variant D** (30 minutes)
   - Stop other variants
   - Start Variant D: `.\RUN_VARIANT.ps1 D`
   - Run JMeter test: `.\RUN_TEST_DOCKER.ps1 D`
   - Fill corresponding cells in T2, T5, T7

### Medium Priority:

3. **Create/Run JOIN-filter Test** (1 hour)
   - Need to create a JMeter test for this scenario
   - Test all three variants
   - Fill T4 table

4. **Create/Run MIXED Test** (1 hour)
   - Need JMeter test with POST/PUT/DELETE operations
   - Test all three variants
   - Complete T5 table

### Lower Priority:

5. **Create/Run HEAVY-body Test** (1 hour)
   - Test with 5KB payloads
   - Fill remaining T2 rows

---

## ğŸ“ˆ Key Findings So Far

### Performance Winner: Variant A (Jersey)
- **+34.7% better throughput** (122 vs 91 RPS)
- **~2x better p95 latency** (251ms vs 431ms)
- **~2.5x better p99 latency** (331ms vs 814ms)
- **More consistent** (max latency: 1.7s vs 10.4s)

### Why is Jersey faster?
Possible reasons (need to verify with JVM metrics):
1. **Lower overhead**: JAX-RS is lighter than Spring MVC
2. **Simpler request pipeline**: Fewer interceptors/filters
3. **Better memory usage**: Less object creation
4. **Optimized serialization**: Jersey's JSON handling

### Concerns with Variant C:
- Latency spikes up to **10.4 seconds** (vs 1.7s for A)
- Could indicate GC pauses or connection pool issues
- Need to check Grafana for memory/GC metrics

---

## ğŸ› ï¸ Quick Commands Reference

### Start a variant:
```powershell
.\RUN_VARIANT.ps1 A  # or C or D
```

### Run JMeter test:
```powershell
cd jmeter
.\RUN_TEST_DOCKER.ps1 A  # or C or D
```

### Check if variant is running:
```powershell
curl http://localhost:8081/actuator/health  # Variant A
curl http://localhost:8082/actuator/health  # Variant C
curl http://localhost:8083/actuator/health  # Variant D
```

### Access monitoring:
- Grafana: http://localhost:3000 (admin/admin)
- Prometheus: http://localhost:9090
- JMeter Reports: `jmeter\results\variant-X-report-XXXXX\index.html`

### Stop all variants:
```powershell
Get-Process java -ErrorAction SilentlyContinue | Stop-Process -Force
```

---

## ğŸ“Š Data Sources

| Table | Data Source | Location |
|-------|-------------|----------|
| T0 | System info | Already filled |
| T1 | Test config | JMX files + manual documentation |
| T2 | JMeter results | `jmeter/results/*/statistics.json` |
| T3 | Grafana/Prometheus | http://localhost:3000 (during test) |
| T4 | JMeter results | JOIN-filter test results |
| T5 | JMeter results | MIXED test results |
| T6 | Logs + JMeter | Error logs + JMeter error reports |
| T7 | Analysis | Calculated from T2-T6 |

---

## ğŸ’¡ Tips for Getting Good Data

1. **Let tests run completely** - Don't interrupt
2. **One variant at a time** - Avoid memory issues
3. **Open Grafana BEFORE starting test** - Capture full metrics
4. **Screenshot Grafana dashboards** - For documentation
5. **Check JMeter HTML reports** - More detailed than JSON
6. **Run tests multiple times** - Verify consistency
7. **Document anomalies** - Note any spikes or errors in T6

---

## âœ… Completion Status

- [x] T0 Configuration: **100%**
- [ ] T1 Scenarios: **50%** (need to verify actual configs)
- [ ] T2 Results: **20%** (1 scenario, 2 variants)
- [ ] T3 JVM Resources: **0%** (need Grafana data)
- [ ] T4 JOIN Details: **0%** (need to create/run test)
- [ ] T5 MIXED Details: **30%** (GET only, 2 variants)
- [ ] T6 Incidents: **0%** (no errors to report yet)
- [ ] T7 Synthesis: **50%** (partial A vs C comparison)

**Overall Progress: ~30%**

---

## ğŸ¯ Recommended Next Action

**Option 1 - Quick Win (30 min):**
Run Variant D test to get A vs C vs D comparison:
```powershell
# Stop current variants
Get-Process java -ErrorAction SilentlyContinue | Stop-Process -Force

# Start Variant D
.\RUN_VARIANT.ps1 D

# In new terminal - run test
cd jmeter
.\RUN_TEST_DOCKER.ps1 D
```

**Option 2 - Complete Analysis (2-3 hours):**
1. Get Grafana metrics for A & C (rerun with monitoring)
2. Test Variant D
3. Create and run JOIN-filter scenario
4. Create and run MIXED scenario
5. Complete all tables

**My Recommendation:** Start with Option 1 to get Variant D data, then move to complete analysis if needed for your project.
